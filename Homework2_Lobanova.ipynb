{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "SBMAPhN7_-cq",
    "outputId": "dccd4788-b525-4bca-9e9f-005245659b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting natasha\n",
      "  Downloading natasha-1.3.0-py3-none-any.whl (34.4 MB)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pymorphy2 in c:\\program files\\python37\\lib\\site-packages (from natasha) (0.8)\n",
      "Collecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Collecting razdel>=0.5.0\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting yargy>=0.14.0\n",
      "  Downloading yargy-0.14.0-py3-none-any.whl (41 kB)\n",
      "Collecting slovnet>=0.3.0\n",
      "  Downloading slovnet-0.4.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python37\\lib\\site-packages (from navec>=0.9.0->natasha) (1.17.3)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\program files\\python37\\lib\\site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\program files\\python37\\lib\\site-packages (from pymorphy2->natasha) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\program files\\python37\\lib\\site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Collecting intervaltree>=3\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0\n",
      "  Downloading sortedcontainers-2.2.2-py2.py3-none-any.whl (29 kB)\n",
      "Using legacy setup.py install for intervaltree, since package 'wheel' is not installed.\n",
      "Installing collected packages: navec, sortedcontainers, intervaltree, ipymarkup, razdel, yargy, slovnet, natasha\n",
      "    Running setup.py install for intervaltree: started\n",
      "    Running setup.py install for intervaltree: finished with status 'done'\n",
      "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.3.0 navec-0.9.0 razdel-0.5.0 slovnet-0.4.0 sortedcontainers-2.2.2 yargy-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script navec-train.exe is installed in 'C:\\Users\\Иришка\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script razdel-ctl.exe is installed in 'C:\\Users\\Иришка\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f1y4sx6AAA3n"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Открываем русский текст (с омонимами)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HweMyD0ehI9Y",
    "outputId": "48a7afff-edd3-4ef3-dee3-3383856bb514"
   },
   "outputs": [],
   "source": [
    "with open('rus.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ты белых лебедей кормила откинув тяжесть черных кос  Я рядом плыл  сошлись кормила  закатный луч был странно кос  Вдруг лебедей метнулась пара  Не знаю  чья была вина  Закат замлел за дымкой пара алея  как поток вина  Мгновенья двигались и стали  лишь ты паришь  свой свет струя  Меж тем в реке   из сизой стали влачится за струей струя  Важно белый корабль  с тысячью  если и не больше  аккуратных одинаковых окошек плыл по воздуху  разрезая холодную хрустальную гладь парков  Весна пришла  Роскошь тонкого стекла фоторамка в виде ананаса   настоящее спасение от плохого настроения   Полковник едет в кузове машины  с ним рядом фельдшер  И тут впереди оказывается запевала  Мелодия крепнет  Она требует марша \n"
     ]
    }
   ],
   "source": [
    "cleant = re.compile('<.*?>')\n",
    "cleantext = re.sub(cleant, ' ', text)\n",
    "cleanre = re.compile('[^а-яёА-ЯЁ ]')\n",
    "cleantext = re.sub(cleanre, ' ', text)\n",
    "print(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Парсим в pymorphy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "I8uOAmivBVRe"
   },
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    text = text.lower()\n",
    "    ls = []\n",
    "    token = word_tokenize(text)\n",
    "    for word in token:\n",
    "        an = morph.parse(word)\n",
    "        optag = an[0].tag\n",
    "        pos = str(optag).split(',')\n",
    "        ls.append(pos[0])\n",
    "    return(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NPRO', 'NOUN', 'NOUN', 'VERB', 'GRND', 'NOUN', 'NOUN', 'NOUN', 'NPRO', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADJF', 'NOUN', 'VERB', 'ADVB', 'NOUN', 'ADVB', 'NOUN', 'VERB', 'NOUN', 'PRCL', 'VERB', 'ADJF', 'VERB', 'NOUN', 'NOUN', 'VERB', 'PREP', 'NOUN', 'NOUN', 'GRND', 'CONJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'CONJ', 'VERB', 'PRCL', 'NPRO', 'VERB', 'ADJF', 'NOUN', 'GRND', 'PREP', 'CONJ', 'PREP', 'NOUN', 'PREP', 'ADJF', 'VERB', 'VERB', 'PREP', 'NOUN', 'GRND', 'ADJS', 'ADJF', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'CONJ', 'PRCL', 'COMP', 'ADJF', 'ADJF', 'NOUN', 'VERB', 'PREP', 'NOUN', 'GRND', 'ADJF', 'ADJF', 'NOUN', 'ADJS', 'NOUN', 'ADJS femn', 'NOUN', 'ADJF', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'ADJF', 'NOUN', 'NOUN', 'VERB', 'PREP', 'NOUN', 'NOUN', 'PREP', 'NPRO', 'NOUN', 'NOUN', 'CONJ', 'ADVB', 'ADVB', 'VERB', 'VERB', 'NOUN', 'VERB', 'NPRO', 'VERB', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "pym = tokenizing(cleantext)\n",
    "for i in pym:\n",
    "    if i == 'NPRO':\n",
    "        i = 'PRON'\n",
    "    elif i == 'GRND':\n",
    "        i = 'VERB'\n",
    "    elif i == 'ADJF':\n",
    "        i = 'ADJ'\n",
    "    elif i == 'PREP':\n",
    "        i = 'ADP'\n",
    "print(pym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Парсим в mystem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myst(text):\n",
    "    m = Mystem()\n",
    "    ms = m.analyze(cleantext)\n",
    "    gr = []\n",
    "    l = []\n",
    "    for i in ms:\n",
    "        if (i.get('text') != ' ') and (i.get('text') != ' \\n') and (i.get('text') != '  ') and (i.get('text') != '   '):\n",
    "            l.append(i)\n",
    "    for k in l:\n",
    "        ana = k.get('analysis')\n",
    "        grpos = ana[0].get('gr')\n",
    "        mys = grpos.split(',')\n",
    "        gr.append(mys[0])\n",
    "    return(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "mys = myst(cleantext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Парсим в Natasha*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "doc = Doc(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'PRON', 'ADV', 'VERB', 'VERB', 'VERB', 'ADJ', 'NOUN', 'AUX', 'ADV', 'ADJ', 'ADV', 'NOUN', 'VERB', 'NOUN', 'PART', 'VERB', 'DET', 'AUX', 'NOUN', 'PROPN', 'VERB', 'ADP', 'NOUN', 'NOUN', 'VERB', 'SCONJ', 'NOUN', 'NOUN', 'PROPN', 'VERB', 'CCONJ', 'VERB', 'PART', 'PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'PROPN', 'PRON', 'ADP', 'NOUN', 'ADP', 'NOUN', 'VERB', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADV', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'SCONJ', 'PART', 'PART', 'NUM', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'PROPN', 'VERB', 'PROPN', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PROPN', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADP', 'PRON', 'ADV', 'NOUN', 'CCONJ', 'ADV', 'ADV', 'VERB', 'VERB', 'PROPN', 'VERB', 'PRON', 'VERB', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "sent = doc.sents[0].morph\n",
    "nat = []\n",
    "rez = []\n",
    "for j in sent:\n",
    "    nat.append(j)\n",
    "n = list(nat[0])\n",
    "for i in n:\n",
    "    rez.append(list(i)[1])\n",
    "print(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [110, 109]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-347-482904886715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'PRON'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PRON'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[1;34m'ADJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PART'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'DET'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'CONJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'CONJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'PART'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'PRON'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'DET'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'PRON'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'CONJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'PART'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PART'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NUM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PROPN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PRON'\u001b[0m \u001b[1;34m'ADV'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'CONJ'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'ADV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PRON'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrez\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [110, 109]"
     ]
    }
   ],
   "source": [
    "correct = ['PRON', 'ADJ', 'NOUN', 'VERB' , 'ADJ' , 'NOUN' , 'ADJ', 'NOUN', 'PRON', 'ADV', 'VERB' , 'VERB' , 'VERB' ,  'ADJ' , 'NOUN' , 'VERB' , 'ADV' , 'ADJ' , 'ADV' , 'NOUN' , 'VERB', 'NOUN', 'PART', 'VERB' , 'DET' , 'VERB' , 'NOUN' , 'NOUN' , 'VERB' , 'ADP' , 'NOUN' , 'NOUN' , 'VERB' , 'CONJ' , 'NOUN' , 'NOUN' , 'NOUN' , 'VERB' , 'CONJ' , 'VERB' , 'PART' , 'PRON' , 'VERB' , 'DET' , 'NOUN' , 'NOUN' , 'ADP' , 'PRON' ,'ADP', 'NOUN', 'ADP', 'NOUN' ,'VERB', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADV', 'ADJ', 'NOUN', 'ADP', 'NOUN' , 'CONJ' , 'PART', 'PART', 'NUM', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN', 'VERB', 'ADJ', 'ADJ' , 'NOUN' , 'NOUN' , 'NOUN' , 'VERB' , 'NOUN' , 'ADJ', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'ADJ', 'NOUN' , 'ADP', 'ADJ', 'NOUN', 'PROPN', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADP', 'PRON' 'ADV' , 'NOUN' , 'CONJ' ,'ADV', 'ADV', 'VERB', 'NOUN' , 'NOUN' , 'VERB', 'PRON', 'VERB', 'NOUN']\n",
    "accuracy_score(rez, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (46.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.45.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\python37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2019.9.11)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\иришка\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Открываем английский текст с омонимами*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng.txt', 'r', encoding='utf-8') as f:\n",
    "    eng = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-329-34d7b8f61d43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting flair\n",
      "  Downloading flair-0.6.1-py3-none-any.whl (331 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\program files\\python37\\lib\\site-packages (from flair) (0.21.3)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "Requirement already satisfied: langdetect in c:\\program files\\python37\\lib\\site-packages (from flair) (1.0.8)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\program files\\python37\\lib\\site-packages (from flair) (4.45.0)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting hyperopt>=0.1.1\n",
      "  Downloading hyperopt-0.2.5-py2.py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\program files\\python37\\lib\\site-packages (from flair) (2.8.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\program files\\python37\\lib\\site-packages (from flair) (3.1.1)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\program files\\python37\\lib\\site-packages (from flair) (3.8.1)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.0-cp37-cp37m-win_amd64.whl (3.5 MB)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting transformers>=3.0.0\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.2-py3-none-any.whl (19 kB)\n",
      "Collecting gdown\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.2-py3-none-any.whl (19 kB)\n",
      "Collecting pytest>=5.3.2\n",
      "  Downloading pytest-6.1.1-py3-none-any.whl (272 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.10.15-cp37-cp37m-win_amd64.whl (272 kB)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch>=1.1.0 (from flair) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch>=1.1.0 (from flair)\n",
      "WARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
