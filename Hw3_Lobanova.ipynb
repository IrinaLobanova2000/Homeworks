{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va5DaqY5L51g"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66lMB8xxLxuV"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmZWKaA-Lxub"
      },
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab2Kg5YgLxuf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U_mfMWZLxui"
      },
      "source": [
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.target_names.unique())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUpKBllALxul"
      },
      "source": [
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "pprint(data[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k32CRxzLxuo"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WP9HUl-Lxur"
      },
      "source": [
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-urMRGGLxuu"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuH10TjaLxux"
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGxewkvLxu2"
      },
      "source": [
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mBCjnQ8tEnN"
      },
      "source": [
        "#Этот блок выдает ошибку \"CalledProcessError\"\n",
        "mallet_path = r'C:\\Users\\Иришка\\Downloads\\mallet-2.0.8' # update this path\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
        "\n",
        "# Show Topics\n",
        "pprint(ldamallet.show_topics(formatted=False))\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sqV_yrwWg10"
      },
      "source": [
        "def best_group():\n",
        "  maxim = 0\n",
        "  number = 0\n",
        "  for i in range(10,30):\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=i, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=5,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    if coherence_lda >= maxim:\n",
        "      maxim = coherence_lda\n",
        "      number = i\n",
        "    return number"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S24aQV3bZzCr"
      },
      "source": [
        "best_group()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88PzpOpRdjfR"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=5,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEOaMSTTakI7"
      },
      "source": [
        "topics = lda_model.show_topics(10, formatted=False)\n",
        "#print(topics[0][1][1])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS7k2hAkqUnR"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKxfpWjKcCDS"
      },
      "source": [
        "text_in_topic = {}\n",
        "n=0\n",
        "for text in texts:\n",
        "  n+=1\n",
        "  count = Counter()\n",
        "  for word in text:\n",
        "    for topic in topics:\n",
        "      id = topic[0]\n",
        "      for i in range(len(topic[1])):\n",
        "        if topic[1][i][0] == word:\n",
        "          count[id]+= topic[1][i][1]\n",
        "  d = dict(count.most_common())\n",
        "  max = 0\n",
        "  i_cor = 0\n",
        "  for i in d:\n",
        "    if d.get(i) > max:\n",
        "      max = d.get(i)\n",
        "      i_cor = i\n",
        "    text_in_topic.update({n : i_cor})\n",
        "#print(text_in_topic[0])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7-oRI6eNLz"
      },
      "source": [
        "groups = {}\n",
        "for i in range(0,10):\n",
        "  lis = []\n",
        "  for key, value in text_in_topic.items():\n",
        "    if value == i:\n",
        "      lis.append(key)\n",
        "    groups.update({i:lis})"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Bl8u9iOfaa"
      },
      "source": [
        "def computeTF(t):\n",
        "  numOfWords = dict.fromkeys(t, 0)\n",
        "  for word in t:\n",
        "    numOfWords[word] += 1\n",
        "  tfDict = {}\n",
        "  textCount = len(t)\n",
        "  for word, count in numOfWords.items():\n",
        "    tfDict[word] = count / float(textCount)\n",
        "  return tfDict"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYpFSr8cQP5O"
      },
      "source": [
        "tfs = {}\n",
        "for i in range (0,10):\n",
        "  num_text = groups[i]\n",
        "  for n in num_text:\n",
        "    need_text = texts[n-1]\n",
        "    get_tf = computeTF(need_text) #список словарей (для каждого текста словарь со значениями)\n",
        "    tfs.update({i: [n , get_tf]})\n",
        "print(tfs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeumRqLtfsT4"
      },
      "source": [
        "import math"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2IKWYO3fyIp"
      },
      "source": [
        "def computeIDF(documents):\n",
        "    N = len(documents[0])\n",
        "    idfDict = dict.fromkeys(documents[0], 0)\n",
        "    for document in documents: \n",
        "        for word, val in document.items():\n",
        "          if val > 0:\n",
        "            if word in idfDict:\n",
        "                idfDict[word] += 1\n",
        "            else:\n",
        "              idfDict.update({word:1})\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4H733Dl2DBg"
      },
      "source": [
        "idfs = {}\n",
        "for i in range(0,10):\n",
        "  num_tex = groups[i]\n",
        "  for n in num_tex:\n",
        "    ls = []\n",
        "    new = []\n",
        "    need_tex = texts[n-1]\n",
        "    numOfWords_ = dict.fromkeys(need_tex, 0)\n",
        "    for word in need_tex:\n",
        "      numOfWords_[word] += 1\n",
        "    ls.append(numOfWords_)\n",
        "    #print(len(ls[0]))\n",
        "  com_idf = computeIDF(ls)\n",
        "  idfs.update({i: com_idf})\n",
        "#print(idfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryQF00ECnHrA"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHm1BjG25c7_"
      },
      "source": [
        "all_tf_idf = []\n",
        "for i in range (0,10):\n",
        "  num = groups[i]\n",
        "  ttfidf = []\n",
        "  for n in num:\n",
        "    need = texts[n-1]\n",
        "    tf_ = tfs[i][1]\n",
        "    idf_ = idfs[i]\n",
        "    sor_words = []\n",
        "    words = Counter()\n",
        "    for word in need:\n",
        "      if word in tf_ and word in idf_:\n",
        "        tf = tf_[word]\n",
        "        idf = idfs[i][word]\n",
        "        tfidf = tf*idf\n",
        "        #print(i,n,word,tfidf)\n",
        "        words.update({word:tfidf})\n",
        "    d = dict(words.most_common(5))\n",
        "    sor_words.append(d)\n",
        "    #print(sor_words[0])\n",
        "    for w in sor_words:\n",
        "      #print(w,n)\n",
        "      for key, value in w.items():\n",
        "        charact = [key , value, n, i]\n",
        "        all_tf_idf.append(charact)\n",
        "#print(all_tf_idf)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVR3r7ZCMblD",
        "outputId": "fe546228-f28f-48e5-ee2e-080ed48c88bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "df = pd.DataFrame(all_tf_idf)\n",
        "df.columns = ['word','tf_idf', 'text_number', 'topic']\n",
        "df"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tf_idf</th>\n",
              "      <th>text_number</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>year</td>\n",
              "      <td>1.028131</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>make</td>\n",
              "      <td>0.411252</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>get</td>\n",
              "      <td>0.370127</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>would</td>\n",
              "      <td>0.370127</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good</td>\n",
              "      <td>0.287877</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42884</th>\n",
              "      <td>enviroleague</td>\n",
              "      <td>1.337965</td>\n",
              "      <td>11277</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42885</th>\n",
              "      <td>youth</td>\n",
              "      <td>1.057157</td>\n",
              "      <td>11277</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42886</th>\n",
              "      <td>program</td>\n",
              "      <td>1.057157</td>\n",
              "      <td>11277</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42887</th>\n",
              "      <td>organization</td>\n",
              "      <td>0.809386</td>\n",
              "      <td>11277</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42888</th>\n",
              "      <td>adult</td>\n",
              "      <td>0.412952</td>\n",
              "      <td>11277</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42889 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               word    tf_idf  text_number  topic\n",
              "0              year  1.028131           18      0\n",
              "1              make  0.411252           18      0\n",
              "2               get  0.370127           18      0\n",
              "3             would  0.370127           18      0\n",
              "4              good  0.287877           18      0\n",
              "...             ...       ...          ...    ...\n",
              "42884  enviroleague  1.337965        11277      9\n",
              "42885         youth  1.057157        11277      9\n",
              "42886       program  1.057157        11277      9\n",
              "42887  organization  0.809386        11277      9\n",
              "42888         adult  0.412952        11277      9\n",
              "\n",
              "[42889 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHBwnbsfvZRh"
      },
      "source": [
        "Coherence score - это показатель, который измеряет оценку одной темы (topic) путем оценки степени семантического сходства между словами в этой теме, обладающими наибольшими весами."
      ]
    }
  ]
}
